# Extended LLM Configuration for config.yaml
# Add this to your existing /home/krawin/code/jarvis/config.yaml

llm:
  # Default provider (current: qwen)
  provider: "qwen"
  
  # Provider configurations
  providers:
    qwen:
      model: "qwen3-coder-plus"
      base_url: "https://portal.qwen.ai/v1"
      temperature: 0.4
      max_tokens: 4000
    
    claude:
      model: "claude-3-5-sonnet-20241022"
      temperature: 0.7
      max_tokens: 4000
    
    gpt:
      model: "gpt-4o"
      temperature: 0.7
      max_tokens: 4000
    
    gemini:
      model: "gemini-2.0-flash-exp"
      temperature: 0.7
      max_tokens: 4000
    
    ollama:
      model: "llama3.2"
      base_url: "http://localhost:11434"
      temperature: 0.7
  
  # Global settings
  timeout: 30
  retry_attempts: 3
